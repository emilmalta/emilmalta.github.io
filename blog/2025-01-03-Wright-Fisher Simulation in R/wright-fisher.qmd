---
title: "Wright-Fisher simulations in R"
author: "Emil Malta"
date: "2025-01-06"
toc: true
description: "This post covers genetic drift simulations in R, how to compare different ways to implement it, and how to visualise it."
filters:
  - line-highlight
execute:
  warning: false
bluesky-comments:
  mute-patterns:
    - "ðŸ“Œ"
  mute-users:
    - "did:plc:1234abcd"
  filter-empty-replies: true
  n-show-init: 3
  n-show-more: 2
draft: true
---

## The Wright Fisher model

Something something. This process is called genetic drift, and explains much of the reduction of genetic variance in a population over time, especially if the population size is small. A common model used to simulate this is called the Wright-Fisher model, which assumes that:

- The size of the population is constant over time.

- The generations don't overlap.

- The change in allele frequencies is a stochastic process.

Allele frequency of each generation comes from sampling a binomial distribution based on frequency of previous generation. Repeat this enough times, and the allele will eventually be be fixed or lost (all individuals modeled have the allele, or the allele is lost entirely).

## Visualising drift

The following code shows my first attempt at implementing the process. This approach is pretty inefficient, and I'll get into why a bit later in the post. 

```{r}
wf_sim <- function(p = .5, n = 50, t = 50) {
  freqs <- c(p)
  for(i in 2:t) {
    freqs <- c(freqs, rbinom(1, size = 2*n, prob = freqs[i - 1]) / (2*n))
  }
  freqs
}
```

Say you have a population of 50 animals, and you're able to measure the frequency of an allele, once every generation. The frequency turns out to be 50% at the first generation, but for every generation that changes a little bit:

```{r}
library(tidyverse)

enframe(wf_sim(), name = "t", value = "p") %>% 
  ggplot(aes(x = t, y = p)) +
  geom_line() +
  scale_y_continuous(labels = scales::percent, limits = c(0,1))
```

Now imagine 100 simulations. Even as every simulation started with the same allele frequency, every trajectory will look different. Some simulation will even have fixed or lost the allele, purely due to chance.

Let's try and illustrate it. This function draws the allele trajectories for every simulation, along with a histogram of the distribution of allele frequencies at the final generation. I use `patchwork` to stitch my plots together:

```{r}
library(patchwork)

draw_sims <- function(sims = 100, sim_function = wf_sim, ...) {
  
  dots <- list(...)
  
  df <- tibble(sim = seq_along(1:sims)) %>% 
    group_by(sim) %>% 
    reframe(
      p = sim_function(...), t = seq_along(p)
    ) %>% 
    ungroup()
  
  p1 <- df %>% 
    ggplot(aes(x = t, y = p, group = sim)) +
    geom_line(alpha = .25) +
    scale_y_continuous(labels = scales::percent, limits = c(0,1)) +
    labs(title = paste(
      names(dots), unlist(dots), sep = " = ", collapse = ", ")
    )
  
  p2 <- df %>% 
    filter(t == max(t)) %>% 
    ggplot(aes(x = p)) +
    geom_histogram(bins = 20, color = "white") +
    scale_x_continuous(
      labels = scales::percent, 
      oob = scales::oob_keep, 
      limits = c(0,1)
    ) +
    coord_flip() +
    theme(axis.text = element_blank(), axis.title = element_blank())
  
  p1 + p2 + plot_layout(design = "AAAAB", axes = 'collect')
}

draw_sims() 
```

This trajectory is dependent on population size. In smaller populations, the trajectories will be all over the place, and the eventual loss or fixation of an allele will be much more likely. The opposite is true for larger populations, and it's going to take a lot longer for the allele to be fixed or disappear.

```{r}
draw_sims(n = 10) / draw_sims(n = 100) 
```

If you're new to R programming, you might notice the use of `...` ellipses in the `draw_sims` function. This is one of the ways R is so outrageous for programmers coming from other languages. This allows us to pass additional arguements to `wf_sim()` with less hard-coding. It's kind of a rabbit hole if I elaborate, so I'm just going to gloss over that for now.

## Improving efficiency

If you play around with the `wf_sim()` function as it's written right now, you'll notice that it's slow, especiqally as `t` increases. This is because this part of the function is actually quite expensive:

```{r}
#| eval: false
#| source-line-numbers: "2,4"
#| class-output: "highlight numberLines"
#| output-line-numbers: "2,4"
wf_sim <- function(p = .5, n = 50, t = 50) {
  freqs <- c(p)
  for(i in 2:t) {
    freqs <- c(freqs, rbinom(1, size = 2*n, prob = freqs[i - 1]) / (2*n))
  }
  freqs
}
```

In each iteration of the loop, R creates a new vector to handle the additional value being appended. This new vector has to copy all of the values from the previous loop, and this repeats when you get to the next iteration and so on.

### Predefining vectors

What you want to do instead is preallocate a vector with the full length before you start looping. The loop should then update a specific index at each iteration. This removes the need to copy all the values every time. Or at least that's what I think is going on.

```{r}
wf_sim_vectorized <- function(p = .5, n = 50, t = 50) {
  freqs <- vector(mode = "numeric", length = t)
  freqs[1] <- p
  for(i in 2:t) {
    freqs[i] <- rbinom(1, size = 2*n, prob = freqs[i - 1]) / (2*n)
  }
  freqs
}
```

Let's test if one approach is faster than the other using `bech::mark`:

```{r}
bench::mark(
  wf_sim(t = 5000), 
  wf_sim_vectorized(t = 5000), 
  check = FALSE
)
```

So yeah, that sounds about right. My hunch is that the time it takes for the algorithm to run increases by $$O(t^2)$$ as the number of generations increases in the first approach, while the vectorized function should run in linear time $$t$$.

The way I was taught to test that is to visualise it by graphs like these:

```{r}
tibble(t = c(50, 100, 500, 1000, 2000, 3000, 4000, 5000)) %>% 
  mutate(benchmarks = map(t, ~{
    bench::mark(
      naive = wf_sim(t = .x),
      vectorized = wf_sim_vectorized(t = .x),
      check = FALSE
    )
  })) %>% 
  unnest(benchmarks) %>% 
  ggplot(aes(x = t, y = median / t, color = as.character(expression))) +
  geom_line() +
  geom_point() +
  labs(color = NULL)
```

Note that the y axis is time to compute divided by $$t$$. If the function grows in linear time with $$t$$, we expect a nice horizontal line, while a function that grows quadratically will have a linear slope like the one shown here.

### Functional programming

Examples like the one above is part of the reason loops get such a bad rap in R. Some very common feedback I got when I was starting out, was that I need to iterate using the `apply` family of functions instead, or the `map` functions of `purrr`, if you prefer the tidyverse style of programming (like I do).

Let's try to write the same function using a function from `purrr`. Because I'm building a vector that uses the initial value, or the result of a previous index, it calls for a function named `accumulate`:

```{r}
wf_sim_tidy <- function(p = .5, n = 50, t = 50) {
  purrr::accumulate(
    .x = vector(mode = "numeric", length = t),
    .f = ~ rbinom(1, 2 * n, .x)/(2 * n), 
    .init = p
  )
}
```

The main advantage of writing code like this is the clearer syntax; not an increase in performance. This is called functional programming, and is a bit less verbose than declaring vectors and writing  loops.

I used to think that functions like these were inherently faster than loops too, but if you actually compare the functions, you'll see that there isn't much to gain in terms of performance compared with our vectorized function. In fact, there's going to be a bit of overhang to `accumulate`, so the tidy version is a bit slower:

```{r}
bench::mark(
  wf_sim(t = 5000),
  wf_sim_vectorized(t = 5000), 
  wf_sim_tidy(t = 5000),
  check = FALSE
)
```

The lesson is that loops are fine. There are some benefits to the syntax in `accumulate`, but they come from the code being more legible and easier to maintain. If you're more worried about performance you might as well stick with the humble loop.

## Understanding Wright-Fisher

The reason I'm writing this code is to gain a better understanding of the factors contributing to genetic drift. That means looking at the distribution of allele frequencies at the end of simulation under varying conditions, namely:

- Population size
- Initial allele frequency
- Presence of mutation rates
- Presence of natural selection

### Population size and initial allele frequency

At this point we've established that initial allele frequency and population size is important to our simulations. To really hammer that idea home, I want to make a high level visualization that shows that in one big plot.

Let's try to look at a few different pairings of allele frequency and population size. I'm choosing 3 values of `p` and 3 values of `n`, with the intent to make the simulations for each possible pairing. I use `crossing()` to make those pairs:

```{r}
crossing(p_init = c(.1, .5, .9), n = c(5, 50, 500))
```

Now let's make 100 simulations for each of these pairs:

```{r}
wf_sims <- crossing(sim = 1:100, p_init = c(.1, .5, .9), n = c(5, 50, 500)) %>% 
  group_by(sim, p_init, n) %>% 
  reframe(
    p = wf_sim_tidy(p = p_init, n = n),
    t = seq_along(p)
  )

wf_sims
```

With a data frame like this, we can show the varying values of `n` and `p` using a faceted ggplot:

```{r}
wf_sims %>% 
  ggplot(aes(x = t, y = p, group = sim)) +
  geom_line(alpha = .2) +
  facet_grid(paste("p_init = ", p_init) ~ paste("n = ", n)) +
  scale_y_continuous(labels = scales::percent) 
```

It's a bit overwhelming to look at all of this, but if you spend some time thinking about the factors for each facet, I hope to build some intuition of trajectories for each condition. Top left shows a tiny population where the initial frequency is very low, while the bottom right shows a big population with a high initial frequency.

The frequencies at the end of the simulations get muddled out, though. Let's look at the resulting distributions:

```{r}
wf_sims %>% 
  filter(t == max(t)) %>% 
  ggplot(aes(x = p)) +
  geom_histogram(color = "white", bins = 20) +
  facet_grid(paste("p_init = ", p_init) ~ paste("n = ", n)) +
  scale_x_continuous(labels = scales::percent) 
```

That gives me the distributions, but the plot I have in my head shows the combined line graphs with the resulting distributions that came with the `draw_sims` function, as I found that helped me understand what is going on. It's hard to show that with regular ggplot facets, so I'm resorting to draw 9 plots using `draw_sims`, and gathering them in a list:

```{r}
wf_plots <- crossing(p_init = c(.1, .5, .9), n = c(5, 50, 500)) %>% 
  rowwise() %>% 
  mutate(plots = list(
    draw_sims(sim_function = wf_sim_tidy, p = p_init, n = n)
    )
  )  %>% 
  pull(plots) 
```

When I have this list, I can wrap them up using the `wrap_plots` function of `patchwork`:

```{r}
wrap_plots(wf_plots) & 
  theme_grey(base_size = 8)
```

There's a lot of high-level things going on right now, but I think it's all in service of understanding the nature of the simulations. 

### Mutation

```{r}
wf_sim_mutation <- function(p = .5, n = 50, t = 50, mut_from = 0, mut_to = 0){
  freqs <- vector(mode = "numeric", length = t)
  freqs[1] <- p
  
  for(i in 2:t) {
    p_mut <- (1 - mut_from) * freqs[i - 1] + mut_to * (1 - freqs[i - 1])
    freqs[i] <- rbinom(1, 2 * n, p_mut) / (2 * n)
  }
  freqs
}
```

### Selection

```{r}
crossing(sim = 1:100, p_init = c(.01, .5, .99), n = c(50, 500, 5000)) %>% 
  group_by(sim, p_init, n) %>% 
  reframe(
    p = wf_sim_mutation(p = p_init, n = n, mut_to = .06, mut_from = .06),
    t = seq_along(p)
  ) -> df

df %>% 
  ggplot(aes(x = t, y = p, group = sim)) +
  geom_line(alpha = .2) +
  facet_grid(paste("p_init: ", p_init) ~ paste("n: ", n)) +
  scale_y_continuous(labels = scales::percent)
```

## Shiny

```{r}
df %>% 
  filter(t == max(t)) %>% 
  ggplot(aes(y = p)) +
  geom_histogram(color = "white") +
  facet_grid(paste("p_init: ", p_init) ~ paste("n: ", n)) +
  scale_y_continuous(labels = scales::percent) 
```

